# AI_NOTES.md — How AI Was Used in This Project

## Overview

This project was built with significant AI assistance (Claude). This document explains exactly where AI was used, what it produced, and what decisions were made independently.

---

## What AI Did

### 1. Code Generation
The majority of the component code was generated by Claude based on detailed prompts describing:
- The exact feature requirements (Q&A with file refs, line numbers, snippets)
- The tech stack choices (React + Vite, Groq, localStorage)
- The design direction (dark terminal aesthetic, IBM Plex Mono)

### 2. Prompt Engineering for Groq
The system prompt in `src/lib/groq.js` was designed collaboratively:
- Structured JSON output format was specified explicitly to prevent hallucination
- `response_format: { type: 'json_object' }` was chosen after understanding that Groq's Llama 3.3 supports it
- The prompt instructs the model to only cite real files from the provided list

### 3. Architecture Decisions (Validated, Not Blindly Accepted)
AI suggested the architecture; here's what was evaluated:

| Decision | AI Suggestion | My Evaluation |
|---|---|---|
| No backend | ✅ Kept | ZIP parsing in-browser is simpler and more private |
| Groq over OpenAI | ✅ Kept | Free tier with no credit card required |
| localStorage over Supabase | ✅ Kept | Zero setup, simpler, sufficient for 10 entries |
| JSZip for ZIP parsing | ✅ Kept | Well-maintained, works in browser |
| GitHub raw API | ✅ Kept | No auth needed for public repos |

---

## What I Understand and Verified

### `src/lib/codeParser.js`
- The `SKIP_DIRS` set correctly excludes `node_modules`, `.git`, `dist`, etc.
- `buildContext()` caps at 80k chars — this fits within Llama 3.3's 128k context window
- Common prefix stripping ensures clean paths like `src/auth.js` not `myrepo-main/src/auth.js`
- GitHub API fetches the tree recursively then fetches raw files in parallel batches of 10

### `src/lib/groq.js`
- Model: `llama-3.3-70b-versatile` — best free model on Groq for code understanding
- Temperature 0.1 — low for deterministic, factual answers
- `response_format: json_object` — enforces structured output
- Error handling covers 401 (bad key), 429 (rate limit), and network failures

### `src/lib/storage.js`
- Max 10 entries enforced with `.slice(0, MAX_HISTORY)`
- try/catch on all localStorage calls (private browsing mode can throw)

---

## What AI Did NOT Do

- Choose the design aesthetic (dark terminal/mono theme was specified in the prompt)
- Choose Groq as the free AI provider (researched independently)
- Decide on the feature set beyond the requirements (tags, refactor suggestions, file search were my additions)
- Write this document (written by me to explain the process)

---

## Honest Assessment of AI Output Quality

- The React component structure was clean and idiomatic
- The Groq prompt required one iteration — first version didn't enforce JSON strictly enough
- The FileTree component had a path-resolution bug (fixed: matching by suffix)
- The GitHub batch fetching logic was correct on first try

---

## Model Used

- **Claude** (claude.ai) for code generation
- **Groq / Llama 3.3 70B** for the in-app AI feature
